\documentclass[12pt,titlepage]{article}

\usepackage{mcgill,palatino,fancyhdr,listings}

\lhead{STAT201A -- Sec. 102}
\chead{HW \#4. }
\rhead{Steven Pollack -- 24112977}
\cfoot{\thepage}

\title{STAT201A -- Sec. 102 \\ Homework \#4. }
\author{Steven Pollack \\ 24112977}
\date{}

\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle

\pagestyle{empty}
\newpage
\pagestyle{fancy}

\paragraph{\#1.} Show that for fixed $n$, the binomial SD is largest when $p=1/2$. Please avoid calculus; the variance is an easily understood function of $p$. 
\begin{proof}
Recall that for a quadratic of the form $b-(x-a)^2$, a maximum is attained at $x=a$ (the point of the vertex). Thus, if we rewrite $p(1-p)$ as
\[
p(1-p) = p(1-p) - \frac{1}{4} + \frac{1}{4} = \frac{1}{4} -\left(p-\frac{1}{2}\right)^2
\]
 we see that our function is a concave down, quadratic with vertex at $p=1/2$. 

Tying this back to $SD(X)$, for $X \sim \text{Binomial}(n,p)$, we have that $SD(X) = \sqrt{npq}$, and thus $SD(X)^2 = \var(X) = np(1-p)$, which is maximized at $p=1/2$. And because $\sqrt{\cdot}$ is a monotonic function therefore optimizing $\sqrt{f(x)}$ is equivalent to optimizing $f(x)$, we see that $SD(X)$ is maximal at $p=1/2$. 
\end{proof}

\newcommand{\g}{\ord{P_{1}(A^{*})-P_{2}(A^{*})}}
\newcommand{\G}{\ord{P_{1}(A_{*})-P_{2}(A_{*})}}
\newcommand{\h}{\ord{P_{1}(B)-P_{2}(B)}}
\newcommand{\dist}{d(P_1,P_2)}
\renewcommand{\S}{\mathcal{S}}

\paragraph{\#2.} Let $\S$ be a finite set and let $P_1$ and $P_2$ be two probability distributions on $\S$. Let $\F$ be the set of all subsets of $\S$. Define the \textit{total variation distance} between $P_1$ and $P_2$ to be 
\[
\dist = \max\set{ A \in \F : \ord{P_1(A) - P_2(A)} }
\]
Thus, $d$ is the largest amount by which the two distributions differ, across all possible events. 

Show that, 
\[
\dist = \frac{1}{2}\sum_{x \in \S} \ord{P_1(x) - P_2(x)}
\]
There are many ways to do this. Here's on, but you are free to use any other.

Let $A^{*} = \set{ x \in \S : P_1(x) > P_2(x) }$, $A_{*} = \set{ x \in \S : P_1(x) < P_2(x) }$, and $A_{*}^{*} = \set{ x \in \S : P_1(x) = P_2(x)}$. The union of these disjoint sets is clearly $\S$. You should have a proof after you've investigated:
\begin{enumerate}
\item[(i)] the relation between $\g$ and $\sum_{x \in A^*} \ord{P_1(x) - P_2(x)}$
\item[(ii)] the relation between $\g$ and $\G$
\item[(iii)] whether the max in the definition of $\dist$ can be greater than $\g$. 
\end{enumerate}

\begin{proof} Defining the following sets:
\begin{align*}
A^{*} &= \set{ x \in \S : P_1(x) > P_2(x) } \\
A_{*} &= \set{ x \in \S : P_1(x) < P_2(x) } \\
A_{*}^{*} &= \set{ x \in \S : P_1(x) = P_2(x)}
\end{align*}
and it's clear that $\S = A^{*} \sqcup A_{*} \sqcup A_{*}^{*}$.  Furthermore, the finite size of $\S$ allows us to write 
\[
P_i(A^{*}) = P_{i}\left( \bigsqcup_{x \in A^{*}} \set{x} \right) = \sum_{x \in A^{*}} P_{i}(x) \qquad (i=1,2)
\]
Hence,
\begin{align*}
\ord{P_{1}(A^{*}) - P_{2}(A^{*})} &= \ord{ \sum_{x\in A^{*}} P_1(x) - \sum_{x \in A^{*}} P_{2}(x)} \\
&= \ord{ \sum_{x \in A^{*}} P_1(x) - P_2(x) } = \sum_{x \in A^{*}} \ord{P_1(x) - P_2(x)} = \sum_{x \in A^{*}} (P_1(x) - P_2(x))
\end{align*}
Similarly, 
\begin{align*}
\ord{P_1(A_{*}) - P_{2}(A_{*})} &= \ord{\sum_{x \in A_{*}} P_{1}(x) - P_{2}(x)} = \sum_{x \in A_{*}} \ord{P_{1}(x) - P_{2}(x)}
\end{align*}
Since $P_{1}(x) - P_{2}(x) = 0$ for all $x \in A_{*}^{*}$, we see that
\begin{align*}
\ord{P_1(A^{*}) - P_{2}(A^{*})}+\ord{P_1(A_{*}) - P_{2}(A_{*})} &= \sum_{x\in A^{*}}\ord{ P_1(x) - P_2(x)} + \sum_{x \in A_{*}} \ord{P_{1}(x) - P_{2}(x)} \\
&= \sum_{x \in \S} \ord{P_{1}(x) - P_{2}(x)}
\end{align*}

Now, suppose $\g > \G$:
\begin{align*}
\g > \G &\EQ P_1(A^*) - P_2(A^*) > P_{2}(A_*) - P_{1}(A_*) \\
&\EQ P_1(A^*) + P_{1}(A_*) > P_2(A^*) + P_2(A_*) \\
&\EQ 1 - P_1(A_{*}^{*}) > 1 - P_2(A_{*}^{*}) \\
&\EQ P_1(A_{*}^{*}) > P_2(A_{*}^{*})
\end{align*}
But, this is impossible, since $P_{1}(x) = P_{2}(x)$ for all $x \in A_{*}^{*}$. Thus, $\g \leq \G$. However, a symmetric argument shows that \[\G \leq \g\] and thus, 
\[
\g = \G
\]

Finally, we establish that $\dist = \g$; Let $B \in \F \setminus\set{A_{*},A^{*}}$ and suppose
\[
\h > \g = \G
\]
Then, 
\[
2 \h > \left(\g + \G\right) = \sum_{x\in\S}\ord{P_{1}(x) - P_{2}(x)}
\]
and hence\footnote{note: $\ord{P_1(B) - P_2(B)} = \ord{\sum_{x\in B}(P_1(x)-P_2(x))} \leq \sum_{x \in B}\ord{P_1(x) -P_2(x)}$}
\[
\h > \sum_{x \in \S\setminus B}\ord{P_{1}(x) - P_{2}(x)} \geq \ord{P_{1}(B^{c}) - P_{2}(B^{c})}
\]
However, 
\[
\ord{P_{1}(B^{c}) - P_{2}(B^{c})} = \ord{(1-P_{1}(B)) - (1-P_{2}(B))} = \ord{P_2(B) - P_1(B)}
\]
Thus, our assumption of strict inequality led us to the (nonsensical) conclusion that
\[
\h > \ord{P_2(B) - P_1(B)}
\]
so our max must be attained at $A^{*}$ and $A_{*}$. Thus, 
\[
\dist = \frac{1}{2} \times 2\dist = \frac{1}{2}\left(\g + \G \right) = \frac{1}{2} \sum_{x\in\S}\ord{P_{1}(x) - P_{2}(x)}
\]
\end{proof}

<<prepare_data, echo=FALSE>>=
calculateDistance <- function(X1,X2) { #L1 distance
  return(sum(abs(X1-X2)))
}

findL1DistanceOfBinomialAndPoisson <- function(n,p) {
  X <- dbinom(x=0:n,size=n,prob=p) # vector of P(X=x), 0 <= x <= n
  Y <- dpois(x=0:n,lambda=n*p) # vector of P(Y=x), 0 <= x <= n
  return(0.5*calculateDistance(X,Y)) #scale according to definition of d(P_1,P_2)
}

findL1DistanceOfBinomialAndNormal <- function(n,p) {
  X <- dbinom(x=0:n,size=n,prob=p)
  Z <- ( pnorm(q=seq(from=0.5, to=n+0.5, by=1),
               mean=n*p, sd=sqrt(n*p*(1-p))
               ) - pnorm(q=seq(from=-0.5,to=n-0.5,by=1),
                         mean=n*p, sd=sqrt(n*p*(1-p)) ) )
  return(0.5*calculateDistance(X,Z)) 
}

findSupNormOfBinomialAndPoisson <- function(n,p) {
  X <- dbinom(x=0:n,size=n,prob=p)
  Y <- dpois(x=0:n,lambda=n*p)
  return(max(abs((X-Y))))
}

findSupNormOfBinomialAndNormal <- function(n,p) {
  X <- dbinom(x=0:n,size=n,prob=p)
  Z <- ( pnorm(q=seq(from=0.5, to=n+0.5, by=1),
               mean=n*p, sd=sqrt(n*p*(1-p)) 
               ) - pnorm(q=seq(from=-0.5, to=n-0.5,by=1),
                         mean=n*p, sd=sqrt(n*p*(1-p)) ) )
  return(max(abs((X-Z))))
}


# set up grid
num_of_trials <- seq(from=20,to=10**3,by=10)
p_values <- seq(from=10**-16, to=0.5,
  length.out=length(num_of_trials))

surfaces <- expand.grid(n=num_of_trials, p=p_values, 
                        KEEP.OUT.ATTRS=FALSE)

#calculate all the distances
surfaces$L1_distance_poisson <- with(surfaces,mapply(
    findL1DistanceOfBinomialAndPoisson, n=n, p=p))

surfaces$sup_norm_poisson <- with(surfaces, mapply(
  findSupNormOfBinomialAndPoisson, n=n, p=p))

surfaces$L1_distance_normal <- with(surfaces, mapply(
  findL1DistanceOfBinomialAndNormal, n=n, p=p))

surfaces$sup_norm_normal <- with(surfaces, mapply(
  findSupNormOfBinomialAndNormal, n=n, p=p))
@

<<label=plot_L1_poisson,echo=FALSE,fig=TRUE,include=FALSE>>=
library(ggplot2)

maximum <- max(surfaces$L1_distance_poisson)
minimum <- min(surfaces$L1_distance_poisson)

d <- ggplot() + layer(data=surfaces, geom="tile",
                      mapping=aes(x=n, y=p, 
                                   fill=L1_distance_poisson))

d <- d + scale_fill_gradientn(
  colours=rainbow(7), breaks=c(seq(from=minimum,to=.1,
                                   length.out=6),maximum))

d <- d + opts(
  panel.background=theme_rect(fill="white", colour="black"),
  panel.grid.major=theme_line(colour = "grey90"),
  title="L1 distance between \n Poisson and Binomial",
  legend.title=theme_blank()) 

print(d)
@

<<label=plot_L1_normal,echo=FALSE,fig=TRUE,include=FALSE>>=
maximum <- max(surfaces$L1_distance_normal)
minimum <- min(surfaces$L1_distance_normal)

d <- ggplot() + layer(data=surfaces, geom="tile",
                 mapping=aes(x=n, y=p, fill=L1_distance_normal))
d <- d + scale_fill_gradientn(
  colours=rainbow(7),breaks=c(seq(from=minimum,to=.1,
                                  length.out=6),maximum)) 
d <- d + opts(
  panel.background=theme_rect(fill="white",colour="black"),
  panel.grid.major=theme_line(colour = "grey90"),
  title="L1 distance between \n Normal and Binomial",
  legend.title=theme_blank())

print(d)
@

<<label=plot_sup_norm_poisson,echo=FALSE,fig=TRUE,include=FALSE>>=
maximum <- max(surfaces$sup_norm_poisson)
minimum <- min(surfaces$sup_norm_poisson)

d <- ggplot() + layer(data=surfaces, geom="tile",
                      mapping=aes(x=n, y=p, fill=sup_norm_poisson)) 

d <- d + scale_fill_gradientn(
  colours=rainbow(7),
  breaks=c(seq(from=minimum,to=.015,length.out=6),.02,.035,maximum))

d <- d + opts(
  panel.background=theme_rect(fill="white",colour="black"),
  panel.grid.major=theme_line(colour = "grey90"),
  title="Max distance between \n Poisson and Binomial",
  legend.title=theme_blank()) 

print(d)
@

<<label=plot_sup_norm_normal,echo=FALSE,fig=TRUE,include=FALSE>>=
maximum <- max(surfaces$sup_norm_normal)
minimum <- min(surfaces$sup_norm_normal)

d <- ggplot() + layer(data=surfaces, geom="tile",
                      mapping=aes(x=n, y=p, fill=sup_norm_normal))

d <- d + scale_fill_gradientn(
  colours=rainbow(7),
  breaks=c(seq(from=minimum,to=.04,length.out=5),.07,.1,.13,maximum))

d <- d + opts(
  panel.background=theme_rect(fill="white",colour="black"),
  panel.grid.major=theme_line(colour = "grey90"),
  legend.position="right",
  title="Max distance between \n Normal and Binomial",
  legend.title=theme_blank())

print(d)
@


\paragraph{\#3.}So a first look at the $L_1$ distance comparisons in figure 1, immediately indicates that there is a convex region in the (half) ($n$,$p$)-plane where the normal approximation is far-superior to the Poisson. Doing some rough linear regression, we can use the function $p = 0.126\exp(-2.32\times10^{-3}n)$ (for $n \geq 20$) as one boundary curve of this region. 

Furthermore, the graphics in figure 2 indicate that this convex region is good, not only for gaging when the normal approximation is $L_1$-superior, but for when it is $L_{\infty}$ superior. Hence, I feel comfortable employing the following ``rule of thumb'': if $(n,p) \in [20,10^{3}]\times[0,1]$, and $p \leq 0.126\exp\set{-2.32\times10^{-3} n}$, then go with the Poisson approximation. Otherwise, use the normal.

Note: $e^{-.00232} \approx 0.998$ and $0.126 \approx 1/8$, so we can roughly peg our rule of thumb to something like
\[
8 p \leq 0.998^{n} %= (1-.002)^{n} = \SUM{j}{1}{n}\binom{n}{j}(-.002)^{j} = (-1)^{j}10^{-3j} 2^j
\]
\begin{figure}[ht!]
\centering
\includegraphics[width=0.6\textwidth, trim = 5mm 0mm 10mm 6mm, clip]{stat201a_hw4-plot_L1_poisson.pdf} \\
\includegraphics[width=0.6\textwidth,trim = 5mm 0mm 10mm 6mm, clip]{stat201a_hw4-plot_L1_normal.pdf} 
\caption{A graphical comparison of the $L_1$ distance between the two distributions as a function of $(n,p)$. Note: red is lower in magnitude than violet.}
\end{figure}

\begin{figure}[ht!]
\centering 
\includegraphics[width=0.6\textwidth, trim = 3mm 0mm 10mm 6mm, clip]{stat201a_hw4-plot_sup_norm_poisson.pdf} \\
\includegraphics[width=0.6\textwidth, trim = 3mm 0mm 10mm 6mm, clip]{stat201a_hw4-plot_sup_norm_normal.pdf}
\caption{A graphical comparison of the $L_{\infty}$ distance between the two distributions as a function of $(n,p)$.}
\end{figure}

\newpage
\appendix{}
\lstinputlisting[language=R,breaklines=True]{../stat201a_hw4.R}
\end{document}