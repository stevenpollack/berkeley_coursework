\documentclass[12pt,titlepage]{article}

\usepackage{mcgill,palatino,fancyhdr}

\lhead{STAT201A -- Sec. 102}
\chead{HW \#7. }
\rhead{Steven Pollack -- 24112977}
\cfoot{\thepage}

\title{STAT201A -- Sec. 102 \\ Homework \#7. }
\author{Steven Pollack \\ 24112977}
\date{}

\begin{document}
\maketitle

\pagestyle{empty}
\newpage
\pagestyle{fancy}
\paragraph{\#1.}
\begin{proof}
\begin{enumerate}
\item[a)] Given $X_i \sim$ Poisson($\lambda_i$), 
\begin{align*}
\psi_{X_i}(t) &= E(e^{tX_i}) \\
&= \SUM{x}{0}{\infty} e^{tx} e^{-\lambda_i} \frac{\lambda_i^x}{x!} \\
&= e^{-\lambda_i} \SUM{x}{0}{\infty} \frac{\left(e^{t}\lambda_i\right)^{x}}{x!} \\
&= e^{-\lambda_i} e^{e^{t} \lambda_i} \\
&= \exp\set{\lambda_{i}(e^{t}-1)} 
\end{align*}
\item[b)] Now, supposing $X_1 \orth X_2$ and using the fact that $\psi_{X+Y} = \psi_X \psi_{Y}$ for $X \orth Y$, we have
\begin{align*}
\psi_{X_1+X_2}(t) &= \psi_{X_1}(t) \psi_{X_2}(t) \\
&= \exp\set{\lambda_{1}(e^{t}-1)}  \exp\set{\lambda_{2}(e^{t}-1)} \\
&= \exp\set{(\lambda_1+\lambda_2)(e^{t}-1)} \\
&= \psi_{Y}(t)
\end{align*}
where $Y \sim$ Poisson($\lambda_1 + \lambda_2$)
\end{enumerate}
\end{proof}

\paragraph{\#2.}
\begin{proof}
Let $X$ have density $f_{X}(x) = \frac{1}{2} e^{-\ord{x}} I_{\R}(x)$. Then, 
\begin{align*}
\psi_{X}(t) &= E(e^{tX}) \\
&= \frac{1}{2}\int_{\R} e^{tx} e^{-\ord{x}} \, dx \\
&= \frac{1}{2} \left( \int_{-\infty}^{0} e^{tx}e^{x} \, dx + \int_{0}^{\infty} e^{tx} e^{-x} \, dx \right) \\
&= \frac{1}{2}\left( \int_{-\infty}^{0} e^{(t+1)x} \, dx + \int_{0}^{\infty} e^{(t-1)x}  \, dx\right)
\end{align*}
Clearly, the first integral will not converge unless $t+1 > 0$ and the second will not converge unless $t-1 < 0$. That is, both will not converge unless $-1 < t < 1$. In which case, 
\[
\psi_{X}(t) = \frac{1}{2}\left(\frac{1}{1+t} + \frac{1}{1-t}\right) = \frac{1}{1-t^2}
\]
Finally, since $-1 < t < 1$ (and therefore $0 < t^2 <1$, we have that 
\[
\frac{1}{1-t^2} = \SUM{k}{0}{\infty} (t^2)^{k}
\]
Hence,
\[
\frac{d^n}{dt^n}\psi_{X}(t) = \SUM{k}{0}{\infty}[2k]_{n-1} t^{2k-n}
\]
Thus,
\[
\psi_{X}^{(n)}(0) = 
\begin{cases}
0 &\text{if $n = 2k+1$ for some $k \in \N$} \\
(2k)! &\text{ if $n = 2k$ for some $k \in \N$}
\end{cases}
\]
\end{proof}

\paragraph{\#3.}
\begin{enumerate}
\item[a)] Recall that for $X > 0$ and $c > 0$, Markov's inequality says that 
\[
P(X \geq c) \leq \frac{E(X)}{c}
\]
Moreover, we have that for any random variable $X$, the transformation $e^{tX}$ is non-negative, and thus applying Markov's inequality to $e^{tX}$, with $c = e^{tx}$ for $t > 0$ and $x \in \R$ yields
\[
P\left( e^{tX} \geq e^{tx} \right) \leq \frac{E(e^{tX})}{e^{tx}}
\]
However, $\exp$ being a monotonically increasing function allows us to write $P(X \geq x ) = P(e^{tX} \geq e^{tx})$, and thus
\[
P(X \geq x) \leq e^{-tx}\psi_{X}(t)
\]
\item[b)] Using the result in class that gives
\[
\psi_{X}(t) = \left(\frac{\lambda}{\lambda - t}\right)^{r}
\]
for $X \sim \Gamma$amma($r,\lambda$), we have that 
\[
P\left(X \geq \frac{2r}{\lambda} \right) \leq e^{-2tr/\lambda} \left(\frac{\lambda}{\lambda-t}\right)^{r} = \left( e^{2t/\lambda}(1-t/\lambda)\right)^{-r}
\]
but since this holds for all $t < \lambda$, setting $t = \lambda/2$ yields the bound
\[
P\left(X \geq \frac{2r}{\lambda} \right) \leq (e(1-1/2))^{-r} = (2/e)^{r}
\]
The bound from Markov is
\[
P\left(X \geq \frac{2r}{\lambda} \right) \leq \frac{E(X)}{2r/\lambda} = \frac{r/\lambda}{2r/\lambda} = \frac{1}{2}
\]
\end{enumerate}

\paragraph{\#4.}
\begin{proof}
\begin{enumerate}
\item[a)] ...
	\begin{enumerate}
	\item[i.] 
	\item[ii.] Since we know that geometric random variables are special cases of negative binomials, and in particular, $X \sim$ NBin($n,p$) implies that $X = \SUM{i}{1}{n} G_i$ where $G_i \iid$ Geo($p$), then via the central limit theorem
	\begin{align*}
	P( X \geq 410 ) &= P\left( \frac{1}{100} \SUM{i}{1}{100} G_i \geq 4.10 \right) \\
	&= P\left( \frac{\bar{G}_{(100)} - 1/p}{\sigma/\sqrt{n}} \geq \frac{4.10 - 4}{\sqrt{(3/4)(1/4)^{-2}}/10} \right) \\
	&\approx P(Z \geq 0.28868) \\
	&= 1-\Phi(0.28868) \\
	&= 38.64\%
	\end{align*}
	Where $G_i \iid$ Geo$(1/4)$ and hence $\sigma = \sqrt{12}$ and $Z \sim$ N(0,1). 
	\end{enumerate}
\end{enumerate}
\end{proof}

\newcommand{\Xn}{\overline{X}_{(n)}}

\paragraph{\#5.}
\begin{proof}
\begin{enumerate}
\item[a)] So, $n$ large puts us in position to use a version of the central limit theorem. 
\begin{align*}
P( \Xn - c_n \leq \mu \leq \Xn + c_n) &= 1 - \left( P(\Xn < \mu - c_n) + P(X_n > \mu + c_n) \right) \\
&= 1 - \left( P\left( \frac{\Xn - \mu}{\sigma/\sqrt{n}} < -\frac{c_n \sqrt{n}}{\sigma} \right) + 1 - P\left( \frac{\Xn - \mu}{\sigma/\sqrt{n}} \leq \frac{c_n \sqrt{n}}{\sigma} \right) \right)\\
&\approx 1 - \left( \Phi\left(-\frac{c_n \sqrt{n}}{\sigma}\right) + 1-\Phi\left(\frac{c_n \sqrt{n}}{\sigma}\right) \right) \\
&= 1 - 2\Phi\left(-\frac{c_n \sqrt{n}}{\sigma}\right) = 1 - 2 \Phi(-z)
\end{align*}
Thus, solving for $z$ such that $1-2\Phi(-z) \approx 95\% \EQ \Phi(-z) \approx 2.5\%$ and letting $c_n = z\sigma/\sqrt{n}$ will give us our desired constant. Using a normal table, we have that $1-\Phi(1.96) = 0.025$, so setting $c_n = 1.96 \sigma/\sqrt{n}$ does the trick.
% 
% Note that \[\Xn - c_n \leq \mu \leq \Xn + c_n \EQ \ord{\mu - \Xn} \leq c_n \EQ \ord{\Xn - \mu} \leq c_n\]
% and that $E(\Xn) = \mu$ and $SD(\Xn) = \sigma/\sqrt{n}$. Thus, Chebyshev says that 
% \[
% P(\ord{\Xn - \mu} \geq k\sigma/\sqrt{n}) \leq \frac{1}{k^2}
% \]
% which means that taking $c_n = k \sigma/\sqrt{n} = \sigma/\sqrt{.05n}$, yields the bound
% \[
% P(\ord{\Xn - \mu} \geq c_n) \leq 5\% \EQ P(\ord{\Xn - \mu} \leq c_n) \geq 95\%
% \]
\item[b)] Using the result that $c_n = 1.96 \sigma/\sqrt{n}$ from above, we know that to obtain a confidence interval of width no larger than .01, $2 c_n \leq 0.01$. That is, 
\[
2 c_n \leq 0.01 \EQ \frac{2\cdot 1.96 \cdot \sigma}{0.01} \leq \sqrt{n} \EQ 392^2 \sigma^2 \leq n
\]
Since this holds for arbitrary $\sigma^2$, it must be that $n \geq \sup_{\sigma^2} 392^2 \sigma^2$ and we saw earlier that for a bernoulli trial with success rate $p$, $\sigma^2$ is maximal at $p = 1/2$. Thus, $n \geq 392^2/4 = 38,416$. 
\end{enumerate}
\end{proof}

\paragraph{\#6.}
\begin{proof}
Given $X_1, X_2, \ldots $ iid with mean $\mu$ (and assuming $\psi_{X_i}$ exists), we have
\[
\psi_{\Xn}(t) = \prod_{i=1}^{n} \psi_{\frac{1}{n}X_i}(t) = \prod_{i=1}^{n} \psi_{X_i}(t/n) = \psi_{X_1}(t/n)^{n}
\]
Expanding $\psi_{X}(t)$ in a Maclaurin series, we have that
\[
\psi_{X}(t) = 1 + \mu t + o(t^2)
\]
Hence, 
\begin{align*}
\psi_{X_1}(t/n)^n &= \left(1 + \frac{\mu t}{n} + o\left(\frac{(\mu t)^2}{n^2}\right)\right)^{n} \\
&= \left(1 + \frac{ \mu t + n o\left(\frac{(\mu t)^2}{n^2}\right)}{n} \right)^{n}
\intertext{Thus,}
\lim_{n\to\infty} \psi_{X_1}(t/n)^n &=  \lim_{n\to\infty}\left(1 + \frac{ \mu t + a_n}{n} \right)^{n} \\
&= e^{\mu t + a}
\intertext{where $a_n \to a$. However,}
a &= \lim_{n\to\infty} n o\left(\frac{(\mu t)^2}{n^2}\right) \\
 &= \lim_{n\to\infty} o\left( \frac{\mu^2 t^2}{n} \right) \\
 &= 0
\end{align*}
Hence, $\psi_{\Xn}(t) = \psi_{X_1}(t/n)^n \to e^{\mu t}$ which is the mgf if the constant $\mu$. 
\end{proof}

\end{document}
